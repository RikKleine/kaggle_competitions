{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description from the challenge page:\n",
    "\n",
    "<i>The Conversation AI team, a research initiative founded by Jigsaw and Google (both a part of Alphabet) are working on tools to help improve online conversation. One area of focus is the study of negative online behaviors, like toxic comments (i.e. comments that are rude, disrespectful or otherwise likely to make someone leave a discussion). So far they’ve built a range of publicly available models served through the Perspective API, including toxicity. But the current models still make errors, and they don’t allow users to select which types of toxicity they’re interested in finding (e.g. some platforms may be fine with profanity, but not with other types of toxic content).\n",
    "\n",
    "In this competition, you’re challenged to build a multi-headed model that’s capable of detecting different types of of toxicity like threats, obscenity, insults, and identity-based hate better than Perspective’s current models. You’ll be using a dataset of comments from Wikipedia’s talk page edits. Improvements to the current model will hopefully help online discussion become more productive and respectful.\n",
    "\n",
    "<b>Disclaimer: the dataset for this competition contains text that may be considered profane, vulgar, or offensive.</b></i>\n",
    "\n",
    "Link to the challenge: https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from sklearn.linear_model import RidgeClassifier, LogisticRegression, RidgeClassifierCV\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from nltk import word_tokenize, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from tqdm import tqdm, tqdm_pandas\n",
    "import string\n",
    "import re\n",
    "import collections\n",
    "\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('data/train.csv')\n",
    "test_data = pd.read_csv('data/test.csv')\n",
    "sample_submission_data = pd.read_csv('data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'comment_text', 'toxic', 'severe_toxic', 'obscene', 'threat',\n",
       "       'insult', 'identity_hate'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()\n",
    "train_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Short exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_examples(data, keyword='severe_toxic', n_samples=4):\n",
    "    if n_samples%2 >0:\n",
    "        n_samples = n_samples + 1\n",
    "    ones_df = data[(data[keyword] == 1) & (data['comment_text'].str.len() < 600)].copy().sample(int(n_samples/2))\n",
    "    zeroes_df = data[(data[keyword] == 0) & (data['comment_text'].str.len() < 600)].copy().sample(int(n_samples/2+1))\n",
    "    merged = zeroes_df.append(ones_df)\n",
    "    \n",
    "    return merged[['comment_text', 'toxic', 'severe_toxic', 'obscene', 'threat',\n",
    "                   'insult', 'identity_hate']].sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I've looked over Tznkai's proposal, but my vote is still to leave things where they are.  The confusion of a move would be much greater than any current confusion that may or may not exist.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Do not add a speedy deletion tag to this as I may expand it.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>u motherfukkin bitch i want to rape you smelly whore stop fucking blocking my account or ill get my nigga homies to came and kidnap and rape you and your family</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>JEW \\n\\nGet the fuck out of here you jewish son of a bitch, I'll rape your fucking family if you don't leave you semite bastard. I will shoot you if you return, because you're a dirty semite, I hope you choke on a fucking bagel, prick. Shalom. We Came In?</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Block Me! I DARE YOU \\n\\nHey, don't tell me what I can and can't do, go ahead and block me, cause if you do, I will have this whole damn website shut down for good. So try me!!!    Wweppvguy</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Even look at Marty Sertich's Wiki page. 2000-2001. HIGH SCHOOL. ROSEVILLE.\\n\\nWhat do you need? A bloody news article stating the aforementioned people went to RAHS? Why can't you uptight assholes just accept personal assurance? What do I have to gain from lying about past alumni? Seriously.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I said that it's a mix of the various bits, anyway, I'm sure we'll email back and fourth a few times before I get permission, so I'll tell then if they specifically ask. However, I feel that it's still fair use. (talk|email)</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                           comment_text  \\\n",
       "0  I've looked over Tznkai's proposal, but my vote is still to leave things where they are.  The confusion of a move would be much greater than any current confusion that may or may not exist.                                                                                                          \n",
       "1  Do not add a speedy deletion tag to this as I may expand it.                                                                                                                                                                                                                                           \n",
       "2  u motherfukkin bitch i want to rape you smelly whore stop fucking blocking my account or ill get my nigga homies to came and kidnap and rape you and your family                                                                                                                                       \n",
       "3  JEW \\n\\nGet the fuck out of here you jewish son of a bitch, I'll rape your fucking family if you don't leave you semite bastard. I will shoot you if you return, because you're a dirty semite, I hope you choke on a fucking bagel, prick. Shalom. We Came In?                                        \n",
       "4  Block Me! I DARE YOU \\n\\nHey, don't tell me what I can and can't do, go ahead and block me, cause if you do, I will have this whole damn website shut down for good. So try me!!!    Wweppvguy                                                                                                         \n",
       "5  Even look at Marty Sertich's Wiki page. 2000-2001. HIGH SCHOOL. ROSEVILLE.\\n\\nWhat do you need? A bloody news article stating the aforementioned people went to RAHS? Why can't you uptight assholes just accept personal assurance? What do I have to gain from lying about past alumni? Seriously.   \n",
       "6  I said that it's a mix of the various bits, anyway, I'm sure we'll email back and fourth a few times before I get permission, so I'll tell then if they specifically ask. However, I feel that it's still fair use. (talk|email)                                                                       \n",
       "\n",
       "   toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0  0      0             0        0       0       0              \n",
       "1  0      0             0        0       0       0              \n",
       "2  1      1             1        1       1       1              \n",
       "3  1      1             1        1       1       1              \n",
       "4  1      0             1        1       0       0              \n",
       "5  1      0             1        0       0       0              \n",
       "6  0      0             0        0       0       0              "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_random_examples(train_data, keyword='threat', n_samples=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean data: tokenize, remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopset = set(stopwords.words('english'))\n",
    "snow = SnowballStemmer('english')\n",
    "WNlemma = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(x, normalization='stemming', remove_stop=False):\n",
    "    \"\"\"Function to preprocess text data. Removes punctuation and numbers. \n",
    "    Lemmatizes or stems words, depending on given parameter. Can also remove \n",
    "    stopwords if specified.\n",
    "    \n",
    "    Args:\n",
    "        x (str): The piece of text to process.\n",
    "        normalization (str): how to normalize words, 'stemming' (default) or 'lemmatization'.\n",
    "        remove_stop (bool): whether to remove stopwords. Default is False.\n",
    "        \n",
    "    Returns:\n",
    "        str: Preprocessed tokens, re-joined with spaces.\n",
    "    \"\"\"\n",
    "    # split text\n",
    "    words = word_tokenize(x)\n",
    "    \n",
    "    # remove punctuation and numbers\n",
    "    words = [word for word in words if word not in string.punctuation and not bool(re.search(r'\\d', word))]\n",
    "    \n",
    "    if normalization == 'stemming':\n",
    "        words = [snow.stem(t) for t in words] # stemming\n",
    "    elif normalization == 'lemmatization':\n",
    "        words = [WNlemma.lemmatize(t.lower()) for t in words] # lemmatize words (advanced stemming)\n",
    "    else:\n",
    "        return 'Invalid parameter for normalization'\n",
    "    \n",
    "    # remove stop words\n",
    "    if remove_stop:\n",
    "        words = [word for word in words if word not in stopset]\n",
    "    \n",
    "    joined_words = ' '.join(words).replace('_', '')\n",
    "    \n",
    "    return joined_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply text cleaning to column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#tqdm.pandas(tqdm()) # for tracking progress\n",
    "#train_data['comment_text'] = train_data['comment_text'].progress_apply(lambda x: clean_text(x, normalization='lemmatization'))\n",
    "\n",
    "train_data['comment_text'] = train_data['comment_text'].apply(lambda x: clean_text(x, normalization='lemmatization'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is a test fuck .. fucking obscene language'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text('This is a __test FUCK 99 !! .. fUcking ObSCENE languages ASsh0l3.', normalization='lemmatization')\n",
    "# note: words with numbers in them currently get dropped. \n",
    "# suggestion: replace numbers in words with letters (e.g. 0 = o, 1 = i, 7 = t, 3 = e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore N-gram frequencies to better estimate appropriate min_df parameter for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(ngram_range=(1,2))\n",
    "train_vect = vect.fit_transform(train_data['comment_text'])\n",
    "dist = np.sum(train_vect, axis=0).tolist()[0]\n",
    "vocab = vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_freq = {}\n",
    "\n",
    "for tag, count in zip(vocab, dist):\n",
    "    ngram_freq[tag]=count\n",
    "    \n",
    "counts = collections.Counter(list(ngram_freq.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 1512510),\n",
       " (2, 277999),\n",
       " (3, 111360),\n",
       " (4, 61127),\n",
       " (5, 39095),\n",
       " (6, 27197),\n",
       " (7, 19998),\n",
       " (8, 15470),\n",
       " (9, 12206),\n",
       " (10, 9907)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# freq, occurrences of freq\n",
    "# e.g. 2045516 words occur one time\n",
    "counts.most_common()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_data['comment_text']\n",
    "y = train_data[['toxic', 'severe_toxic', 'obscene', 'threat',\n",
    "       'insult', 'identity_hate']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline a few models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_clf = Pipeline([('vect', CountVectorizer(ngram_range=(1,2), min_df=4, max_df=0.5, max_features=50000)),\n",
    "                    ('tfidf', TfidfTransformer(use_idf=True)),\n",
    "                    ('clf', OneVsRestClassifier(MultinomialNB(alpha=0.01), n_jobs=-1))])\n",
    "\n",
    "SVC_clf = Pipeline([('vect', CountVectorizer(ngram_range=(1,2), min_df=4, max_df=0.5, max_features=50000)),\n",
    "                         ('tfidf', TfidfTransformer(use_idf=True)),\n",
    "                       ('clf', OneVsRestClassifier(SVC(C=10, probability=True), n_jobs=-1))])\n",
    "\n",
    "logistic_clf = Pipeline([('vect', CountVectorizer(ngram_range=(1,2), min_df=4, max_df=0.5, max_features=15000)),\n",
    "                         ('tfidf', TfidfTransformer(use_idf=True)),\n",
    "                       ('clf', OneVsRestClassifier(LogisticRegression(C=0.1, class_weight='balanced'), n_jobs=-1))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters, specify for which part of pipeline with prefix, e.g. 'vect__'\n",
    "SVC_params = {'vect__ngram_range': [(1,2)],\n",
    "              'tfidf__use_idf': [True],\n",
    "              'clf__estimator__C':[0.1, 1, 10]}\n",
    "\n",
    "logistic_params = {#'vect__ngram_range': [(1,2)],\n",
    "                   'vect__min_df': [3, 4, 5, 6],\n",
    "                   'vect__max_df': [0.3, 0.4, 0.5, 0.6],\n",
    "                   #'vect__max_features': [25000, 50000, 100000, None],\n",
    "                    #'vect__max_features': [5000, 7500, 10000, 12500],\n",
    "                  #'tfidf__use_idf': [True],\n",
    "                  #'clf__estimator__C':[0.1, 0.3, 0.6, 1, 3],\n",
    "                  #'clf__estimator__class_weight':['balanced', None],\n",
    "                  #'clf__estimator__penalty':['l1', 'l2']\n",
    "                  }\n",
    "\n",
    "NB_params = {'vect__ngram_range': [(1,2)],\n",
    "              'tfidf__use_idf': [True],\n",
    "              'clf__estimator__alpha':[0.1, 1, 10]}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "TO_TEST = logistic_clf\n",
    "PARAMS = logistic_params\n",
    "\n",
    "gs_clf = GridSearchCV(TO_TEST, PARAMS, scoring='roc_auc', n_jobs=-1, verbose=1, return_train_score=True).fit(X, y)\n",
    "results = pd.DataFrame(gs_clf.cv_results_).sort_values('rank_test_score')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New pipeline with optimal parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=0.3, max_features=25000, min_df=6,\n",
       "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
       "        stri...None,\n",
       "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False),\n",
       "          n_jobs=-1))])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Pipeline([('vect', CountVectorizer(ngram_range=(1,2), min_df=6, max_df=0.3, max_features=25000)),\n",
    "                         ('tfidf', TfidfTransformer(use_idf=True)),\n",
    "                       ('clf', OneVsRestClassifier(LogisticRegression(C=0.1, class_weight='balanced'), n_jobs=-1))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models to test\n",
    "models = {'Logistic regression': logistic_clf,\n",
    "          #'SVC': SVC_clf,\n",
    "         #'Naïve Bayes': NB_clf\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Benchmark score (in case of all 0 predictions):\n",
    "pred = np.zeros(y_test.shape)\n",
    "roc_auc_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Logistic regression...\n",
      "Logistic regression train ROC_AUC score: 0.9553583845455703\n",
      "Logistic regression test ROC_AUC score: 0.9066418804422969\n",
      "Logistic regression cross validation ROC_AUC score on 5 folds: 0.9765465005353489\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model_name, model in models.items():\n",
    "    print('Training {}...'.format(model_name))\n",
    "    clf = model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = clf.predict(X_train)\n",
    "    print('{} train ROC_AUC score: {}'.format(model_name, roc_auc_score(y_train, y_pred)))\n",
    "    \n",
    "    y_pred = clf.predict(X_test)\n",
    "    print('{} test ROC_AUC score: {}'.format(model_name, roc_auc_score(y_test, y_pred)))\n",
    "    print('{} cross validation ROC_AUC score on 5 folds: {}'.format(model_name, cross_val_score(model, X, y, scoring='roc_auc', cv=5, n_jobs=-1).mean()))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation log"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Base model:\n",
    "logistic_clf = Pipeline([('vect', CountVectorizer(ngram_range=(1,2), min_df=4, max_df=0.5)),\n",
    "                         ('tfidf', TfidfTransformer(use_idf=True)),\n",
    "                       ('clf', OneVsRestClassifier(LogisticRegression(C=10, class_weight='balanced'), n_jobs=-1))])\n",
    "                       \n",
    "Training Logistic regression... \n",
    "Logistic regression train score: 0.9962698309423336\n",
    "Logistic regression test score: 0.8405576601146745\n",
    "\n",
    "Added parameter of max_features at 50000 to CountVectorizer:\n",
    "Training Logistic regression...\n",
    "Logistic regression train score: 0.9943029355921243\n",
    "Logistic regression test score: 0.8474675853683019                      \n",
    "\n",
    "Low regularization with C=100:\n",
    "Training Logistic regression...\n",
    "Logistic regression train score: 0.9979831115997314\n",
    "Logistic regression test score: 0.8037867925850946\n",
    "\n",
    "Higher regularization with C=1:\n",
    "Training Logistic regression...\n",
    "Logistic regression train score: 0.9853433595795452\n",
    "Logistic regression test score: 0.8915858868129986\n",
    "\n",
    "Even higher regularization with C=0.1:\n",
    "Training Logistic regression...\n",
    "Logistic regression train score: 0.9573338881138563\n",
    "Logistic regression test score: 0.9002644623375343\n",
    "\n",
    "Absurdly high regularization with C=0.01:\n",
    "Training Logistic regression...\n",
    "Logistic regression train score: 0.9156204424937071\n",
    "Logistic regression test score: 0.8828513713932437\n",
    "\n",
    "Reg C=0.1, max_features=25000, min_df=3\n",
    "Training Logistic regression...\n",
    "Logistic regression train ROC_AUC score: 0.9560426728378003\n",
    "Logistic regression test ROC_AUC score: 0.9039801868847009\n",
    "\n",
    "max_features=12500\n",
    "Training Logistic regression...\n",
    "Logistic regression train ROC_AUC score: 0.9537134328359774\n",
    "Logistic regression test ROC_AUC score: 0.9083269187748823\n",
    "Logistic regression cross validation ROC_AUC score on 5 folds: 0.975445455672514\n",
    "\n",
    "max_features=15000\n",
    "Training Logistic regression...\n",
    "Logistic regression train ROC_AUC score: 0.9545163453977098\n",
    "Logistic regression test ROC_AUC score: 0.9078888840008452\n",
    "Logistic regression cross validation ROC_AUC score on 5 folds: 0.9756952727924262"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final parameter tuning (manual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross_val_score with C=0.1, max_features=25000, max_df=0.3, min_df=6:  0.977167248668523\n",
      "Cross_val_score with C=0.1, max_features=25000, max_df=0.4, min_df=5:  0.9771515047562769\n"
     ]
    }
   ],
   "source": [
    "logistic_clf = Pipeline([('vect', CountVectorizer(ngram_range=(1,2), min_df=6, max_df=0.3, max_features=25000)),\n",
    "                         ('tfidf', TfidfTransformer(use_idf=True)),\n",
    "                       ('clf', OneVsRestClassifier(LogisticRegression(C=0.1, class_weight='balanced'), n_jobs=-1))])\n",
    "\n",
    "print('Cross_val_score with C=0.1, max_features=25000, max_df=0.3, min_df=6: ', cross_val_score(logistic_clf, X, y, scoring='roc_auc', cv=5, n_jobs=-1).mean())\n",
    "\n",
    "logistic_clf = Pipeline([('vect', CountVectorizer(ngram_range=(1,2), min_df=5, max_df=0.3, max_features=25000)),\n",
    "                         ('tfidf', TfidfTransformer(use_idf=True)),\n",
    "                       ('clf', OneVsRestClassifier(LogisticRegression(C=0.1, class_weight='balanced'), n_jobs=-1))])\n",
    "\n",
    "print('Cross_val_score with C=0.1, max_features=25000, max_df=0.4, min_df=5: ', cross_val_score(logistic_clf, X, y, scoring='roc_auc', cv=5, n_jobs=-1).mean())\n",
    "\n",
    "#Cross_val_score with C=0.1:  0.9759436463859725\n",
    "#Cross_val_score with C=0.01:  0.9635830622046198\n",
    "\n",
    "#Cross_val_score with C=0.1, max_features=25000, max_df=0.5, min_df=4:  0.9760136616571229\n",
    "#Cross_val_score with C=0.1, max_features=50000, max_df=0.5, min_df=4:  0.9759436463859725\n",
    "\n",
    "#Cross_val_score with C=0.1, max_features=25000, max_df=0.3, min_df=4:  0.9762885351187846\n",
    "#Cross_val_score with C=0.1, max_features=25000, max_df=0.4, min_df=4:  0.9760047347166096\n",
    "\n",
    "#Cross_val_score with C=0.1, max_features=25000, max_df=0.3, min_df=6:  0.9763128416184881\n",
    "#Cross_val_score with C=0.1, max_features=25000, max_df=0.3, min_df=5:  0.976302536076927\n",
    "\n",
    "# Same test, but with correction in clean_text function (fixed lowercase issue):\n",
    "# Cross_val_score with C=0.1, max_features=25000, max_df=0.3, min_df=6:  0.977167248668523\n",
    "# Cross_val_score with C=0.1, max_features=25000, max_df=0.4, min_df=5:  0.9771515047562769"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do: write automated test function (start with default, set params to best so far, test one param per iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declare model with final parameters to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = Pipeline([('vect', CountVectorizer(ngram_range=(1,2), min_df=6, max_df=0.3, max_features=25000)),\n",
    "                         ('tfidf', TfidfTransformer(use_idf=True)),\n",
    "                       ('clf', OneVsRestClassifier(LogisticRegression(C=0.1, class_weight='balanced'), n_jobs=-1))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions for submission and save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: submission should be probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean submission data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yo bitch Ja Rule is more succesful then you'll ever be whats up with you and hating you sad mofuckas...i should bitch slap ur pethedic white faces and get you to kiss my ass you guys sicken me. Ja rule is about pride in da music man. dont diss that shit on him. and nothin is wrong bein like tupac he was a brother too...fuckin white boys get things right next time.,\n"
     ]
    }
   ],
   "source": [
    "print(test_data.comment_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean test data\n",
    "#test_data['comment_text'] = test_data['comment_text'].progress_apply(lambda x: clean_text(x, normalization='lemmatization'))\n",
    "test_data['comment_text'] = test_data['comment_text'].apply(lambda x: clean_text(x, normalization='lemmatization'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yo bitch ja rule is more succesful then you 'll ever be whats up with you and hating you sad mofuckas ... i should bitch slap ur pethedic white face and get you to kiss my as you guy sicken me ja rule is about pride in da music man dont dis that shit on him and nothin is wrong bein like tupac he wa a brother too ... fuckin white boy get thing right next time.\n"
     ]
    }
   ],
   "source": [
    "print(test_data.comment_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001cee341fdb12</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000247867823ef7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id  toxic  severe_toxic  obscene  threat  insult  \\\n",
       "0  00001cee341fdb12  0.5    0.5           0.5      0.5     0.5      \n",
       "1  0000247867823ef7  0.5    0.5           0.5      0.5     0.5      \n",
       "\n",
       "   identity_hate  \n",
       "0  0.5            \n",
       "1  0.5            "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_submission_data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train final model and predict probabilities for the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_final = final_model.fit(X, y).predict_proba(test_data['comment_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.DataFrame(y_pred_final, columns=y_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.995390</td>\n",
       "      <td>0.942465</td>\n",
       "      <td>0.996497</td>\n",
       "      <td>0.866189</td>\n",
       "      <td>0.987634</td>\n",
       "      <td>0.947539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.116185</td>\n",
       "      <td>0.063854</td>\n",
       "      <td>0.073821</td>\n",
       "      <td>0.039130</td>\n",
       "      <td>0.103398</td>\n",
       "      <td>0.085246</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      toxic  severe_toxic   obscene    threat    insult  identity_hate\n",
       "0  0.995390  0.942465      0.996497  0.866189  0.987634  0.947539     \n",
       "1  0.116185  0.063854      0.073821  0.039130  0.103398  0.085246     "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge predictions with IDs, explore the outcome and confirm correct shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.concat([test_data['id'], predictions], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID 00001cee341fdb12:\n",
      " yo bitch ja rule is more succesful then you 'll ever be whats up with you and hating you sad mofuckas ... i should bitch slap ur pethedic white face and get you to kiss my as you guy sicken me ja rule is about pride in da music man dont dis that shit on him and nothin is wrong bein like tupac he wa a brother too ... fuckin white boy get thing right next time. \n",
      "\n",
      "ID 0000247867823ef7:\n",
      " == from rfc == the title is fine a it is imo\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001cee341fdb12</td>\n",
       "      <td>0.995390</td>\n",
       "      <td>0.942465</td>\n",
       "      <td>0.996497</td>\n",
       "      <td>0.866189</td>\n",
       "      <td>0.987634</td>\n",
       "      <td>0.947539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000247867823ef7</td>\n",
       "      <td>0.116185</td>\n",
       "      <td>0.063854</td>\n",
       "      <td>0.073821</td>\n",
       "      <td>0.039130</td>\n",
       "      <td>0.103398</td>\n",
       "      <td>0.085246</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id     toxic  severe_toxic   obscene    threat    insult  \\\n",
       "0  00001cee341fdb12  0.995390  0.942465      0.996497  0.866189  0.987634   \n",
       "1  0000247867823ef7  0.116185  0.063854      0.073821  0.039130  0.103398   \n",
       "\n",
       "   identity_hate  \n",
       "0  0.947539       \n",
       "1  0.085246       "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('ID 00001cee341fdb12:\\n',test_data.comment_text[0], '\\n')\n",
    "print('ID 0000247867823ef7:\\n',test_data.comment_text[1])\n",
    "submission.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(153164, 7)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save predictions to .csv, ready for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_PATH = './data/submission_simple.csv'\n",
    "submission.to_csv(TARGET_PATH, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My highest public leaderboard score on Kaggle: 0.0.9723"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus: most important words per label "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show 10 ten most important words, and 10 least important words per label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_estimators = final_model.named_steps['clf'].estimators_\n",
    "vocab = final_model.named_steps['vect'].vocabulary_\n",
    "index_to_words = {value: key for key,value in vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current label: toxic\n",
      "Top 10 most toxic words:\n",
      "('fuck', 9.295420547748519)\n",
      "('fucking', 8.01926826032406)\n",
      "('stupid', 7.775941645246558)\n",
      "('idiot', 7.415786121096115)\n",
      "('shit', 7.325892787270488)\n",
      "('suck', 6.230368382977478)\n",
      "('as', 5.351426184985407)\n",
      "('asshole', 5.212978738540356)\n",
      "('crap', 5.074895273248084)\n",
      "('dick', 5.033463892230288)\n",
      "\n",
      "Top 10 least toxic words:\n",
      "('but', -2.018151832647545)\n",
      "('at', -2.0247040154355096)\n",
      "('source', -2.086360135356195)\n",
      "('thank you', -2.116636549977007)\n",
      "('thank', -2.1968729718784465)\n",
      "('utc', -2.29552975080801)\n",
      "('article', -2.5343513950197027)\n",
      "('please', -2.843646044913631)\n",
      "('talk', -3.364017510262568)\n",
      "('thanks', -3.3772175055523883)\n",
      "\n",
      "\n",
      "Current label: severe_toxic\n",
      "Top 10 most severe_toxic words:\n",
      "('fucking', 9.139312853190223)\n",
      "('fuck', 9.003776593117689)\n",
      "('bitch', 5.724100840073539)\n",
      "('asshole', 5.601583500585075)\n",
      "('shit', 5.515879511526424)\n",
      "('suck', 5.185368550320496)\n",
      "('dick', 4.986553179438271)\n",
      "('as', 4.767529613564076)\n",
      "('fucker', 4.682142091935353)\n",
      "('faggot', 4.530953764827909)\n",
      "\n",
      "Top 10 least severe_toxic words:\n",
      "('in the', -1.6746419892776758)\n",
      "('source', -1.738252240617751)\n",
      "('of the', -1.8500093747698214)\n",
      "('there', -1.8582456217102608)\n",
      "('utc', -2.001014099248806)\n",
      "('wa', -2.0506550946943873)\n",
      "('but', -2.404000912719334)\n",
      "('article', -2.860712515902628)\n",
      "('please', -2.9281463703048565)\n",
      "('talk', -3.2552288846166895)\n",
      "\n",
      "\n",
      "Current label: obscene\n",
      "Top 10 most obscene words:\n",
      "('fuck', 13.223490967219389)\n",
      "('fucking', 11.868701017449533)\n",
      "('shit', 9.450492334569738)\n",
      "('as', 7.950024521486139)\n",
      "('asshole', 7.759880020605825)\n",
      "('bitch', 7.3865568274971745)\n",
      "('suck', 7.158286369533261)\n",
      "('dick', 7.063198594149018)\n",
      "('bullshit', 6.900709323924862)\n",
      "('cunt', 6.165587456573396)\n",
      "\n",
      "Top 10 least obscene words:\n",
      "('of the', -1.5060941635622809)\n",
      "('help', -1.519701585130239)\n",
      "('wp', -1.6565393188380284)\n",
      "('but', -1.7058897117673193)\n",
      "('source', -1.7546991413423325)\n",
      "('please', -1.784296135090074)\n",
      "('utc', -2.0678285091645074)\n",
      "('article', -2.2777276234019617)\n",
      "('thanks', -2.445082770101081)\n",
      "('talk', -3.013271066330566)\n",
      "\n",
      "\n",
      "Current label: threat\n",
      "Top 10 most threat words:\n",
      "('die', 11.065920449442187)\n",
      "('kill', 10.330251318966585)\n",
      "('will', 8.302917376735966)\n",
      "('death', 7.228693951467864)\n",
      "('dead', 5.083901993062509)\n",
      "('as', 4.961303798987389)\n",
      "('your', 4.7903799258458415)\n",
      "('shoot', 4.687349196093649)\n",
      "('ll', 4.614381741903211)\n",
      "('burn', 4.497871618992961)\n",
      "\n",
      "Top 10 least threat words:\n",
      "('an', -1.8076881445551742)\n",
      "('source', -1.907715668955318)\n",
      "('but', -1.9669124977598216)\n",
      "('have', -1.9801752219477986)\n",
      "('thanks', -2.0452088432114204)\n",
      "('page', -2.1520043043094366)\n",
      "('of the', -2.3911342594424454)\n",
      "('talk', -2.565896522619443)\n",
      "('wa', -2.649390488679146)\n",
      "('article', -2.873212010813494)\n",
      "\n",
      "\n",
      "Current label: insult\n",
      "Top 10 most insult words:\n",
      "('idiot', 9.52258369139924)\n",
      "('fuck', 8.378387292597823)\n",
      "('stupid', 7.976445542723419)\n",
      "('fucking', 7.796830579351725)\n",
      "('asshole', 7.087061825100428)\n",
      "('bitch', 6.724994000586908)\n",
      "('faggot', 5.812367446946604)\n",
      "('moron', 5.808426101503706)\n",
      "('suck', 5.698129432997204)\n",
      "('as', 5.607588988078873)\n",
      "\n",
      "Top 10 least insult words:\n",
      "('wp', -1.8764930672516702)\n",
      "('source', -1.9279062409088272)\n",
      "('of the', -1.9779463686918297)\n",
      "('there', -2.062270772674656)\n",
      "('but', -2.1049339220459125)\n",
      "('utc', -2.1647299552808525)\n",
      "('please', -2.43047154962021)\n",
      "('thanks', -2.523148189232499)\n",
      "('article', -2.715252936960432)\n",
      "('talk', -4.007443679236265)\n",
      "\n",
      "\n",
      "Current label: identity_hate\n",
      "Top 10 most identity_hate words:\n",
      "('gay', 10.563333363477211)\n",
      "('nigger', 9.425335063149948)\n",
      "('faggot', 8.293635716519754)\n",
      "('jew', 7.304969481126937)\n",
      "('homosexual', 7.225911584462261)\n",
      "('nazi', 5.947097742252217)\n",
      "('nigga', 5.57374158174309)\n",
      "('homo', 5.2522203676265695)\n",
      "('fucking', 5.125731846074617)\n",
      "('racist', 4.8568169496771905)\n",
      "\n",
      "Top 10 least identity_hate words:\n",
      "('utc', -1.7082552959656576)\n",
      "('editor', -1.730580876489052)\n",
      "('page', -1.7384664672733614)\n",
      "('please', -1.7676204028144784)\n",
      "('wa', -1.7717673571101675)\n",
      "('source', -1.8160663087183218)\n",
      "('there', -2.0851441959683803)\n",
      "('thanks', -2.0882356020663386)\n",
      "('article', -3.3257321533000996)\n",
      "('talk', -3.425672763453652)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for index, label in enumerate(y.columns):\n",
    "    print('Current label: {}'.format(label))\n",
    "    words = {}\n",
    "    coefs = all_estimators[index].coef_[0]\n",
    "    for key in index_to_words.keys():\n",
    "        words[index_to_words[key]] = coefs[key]\n",
    "    words = sorted(words.items(), key=lambda x:x[1], reverse=True)\n",
    "    top_5_most_important = words[:10]\n",
    "    top_5_least_important = words[-10:]\n",
    "    print('Top 10 most {} words:'.format(label))\n",
    "    for pair in top_5_most_important:\n",
    "        print(pair)\n",
    "    print('')\n",
    "    print('Top 10 least {} words:'.format(label))\n",
    "    for pair in top_5_least_important:\n",
    "        print(pair)\n",
    "    print('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
