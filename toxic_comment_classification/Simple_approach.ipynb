{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains a simple logistic regression approach to solve a multilabel classification problem. The dataset and challenge is provided on Kaggle by Jigsaw. The jist of the competition is as follows:\n",
    "\n",
    "\"In this competition, you’re challenged to build a multi-headed model that’s capable of detecting different types of of toxicity like threats, obscenity, insults, and identity-based hate better than Perspective’s current models. You’ll be using a dataset of comments from Wikipedia’s talk page edits. Improvements to the current model will hopefully help online discussion become more productive and respectful.\"\n",
    "\n",
    "Link: https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from sklearn.linear_model import RidgeClassifier, LogisticRegression, RidgeClassifierCV\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from nltk import word_tokenize, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from tqdm import tqdm, tqdm_pandas\n",
    "import string\n",
    "import re\n",
    "import gensim\n",
    "import collections\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('data/train.csv')\n",
    "test_data = pd.read_csv('data/test.csv')\n",
    "sample_submission_data = pd.read_csv('data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Short exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()\n",
    "train_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(train_data.comment_text[5000])\n",
    "print(train_data.iloc[5000][['toxic', 'severe_toxic', 'obscene', 'threat',\n",
    "       'insult', 'identity_hate']],'\\n')\n",
    "\n",
    "print(train_data.comment_text[4000])\n",
    "print(train_data.iloc[4000][['toxic', 'severe_toxic', 'obscene', 'threat',\n",
    "       'insult', 'identity_hate']],'\\n')\n",
    "\n",
    "print(train_data.comment_text[3000])\n",
    "print(train_data.iloc[3000][['toxic', 'severe_toxic', 'obscene', 'threat',\n",
    "       'insult', 'identity_hate']],'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean data: tokenize, remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopset = set(stopwords.words('english'))\n",
    "snow = SnowballStemmer('english')\n",
    "WNlemma = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(x, normalization='stemming', remove_stop=False):\n",
    "    \"\"\"Function to preprocess text data. Removes punctuation and numbers. \n",
    "    Lemmatizes or stems words, depending on given parameter. Can also remove \n",
    "    stopwords if specified.\n",
    "    \n",
    "    Args:\n",
    "        x (str): The piece of text to process.\n",
    "        normalization (str): how to normalize words, 'stemming' (default) or 'lemmatization'.\n",
    "        remove_stop (bool): whether to remove stopwords. Default is False.\n",
    "        \n",
    "    Returns:\n",
    "        str: Preprocessed tokens, re-joined with spaces.\n",
    "    \"\"\"\n",
    "    # split text\n",
    "    words = word_tokenize(x)\n",
    "    \n",
    "    # remove punctuation and numbers\n",
    "    words = [word for word in words if word not in string.punctuation and not bool(re.search(r'\\d', word))]\n",
    "    \n",
    "    if normalization == 'stemming':\n",
    "        words = [snow.stem(t) for t in words] # stemming\n",
    "    elif normalization == 'lemmatization':\n",
    "        words = [WNlemma.lemmatize(t.lower()) for t in words] # lemmatize words (advanced stemming)\n",
    "    else:\n",
    "        return 'Invalid parameter for normalization'\n",
    "    \n",
    "    # remove stop words\n",
    "    if remove_stop:\n",
    "        words = [word for word in words if word not in stopset]\n",
    "    \n",
    "    joined_words = ' '.join(words).replace('_', '')\n",
    "    \n",
    "    return joined_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply text cleaning to column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#tqdm.pandas(tqdm()) # for tracking progress (use progress_apply in code below)\n",
    "train_data['comment_text'] = train_data['comment_text'].apply(lambda x: clean_text(x, normalization='lemmatization'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "clean_text('This is a __test FUCK 99 !! .. fUcking ObSCENE languages shitz0r.', normalization='lemmatization')\n",
    "# note: words with numbers in them currently get dropped. \n",
    "# suggestion: replace numbers in words with letters (e.g. 0 = o, 1 = i, 7 = t, 3 = e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore N-gram frequencies to better estimate appropriate min_df parameter for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(ngram_range=(1,2))\n",
    "train_vect = vect.fit_transform(train_data['comment_text'])\n",
    "dist = np.sum(train_vect, axis=0).tolist()[0]\n",
    "vocab = vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_freq = {}\n",
    "\n",
    "for tag, count in zip(vocab, dist):\n",
    "    ngram_freq[tag]=count\n",
    "    \n",
    "counts = collections.Counter(list(ngram_freq.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freq, occurrences of freq\n",
    "# e.g. 2045516 words occur one time\n",
    "counts.most_common()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_data['comment_text']\n",
    "y = train_data[['toxic', 'severe_toxic', 'obscene', 'threat',\n",
    "       'insult', 'identity_hate']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline a few models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_clf = Pipeline([('vect', CountVectorizer(ngram_range=(1,2), min_df=4, max_df=0.5, max_features=50000)),\n",
    "                    ('tfidf', TfidfTransformer(use_idf=True)),\n",
    "                    ('clf', OneVsRestClassifier(MultinomialNB(alpha=0.01), n_jobs=-1))])\n",
    "\n",
    "SVC_clf = Pipeline([('vect', CountVectorizer(ngram_range=(1,2), min_df=4, max_df=0.5, max_features=50000)),\n",
    "                         ('tfidf', TfidfTransformer(use_idf=True)),\n",
    "                       ('clf', OneVsRestClassifier(SVC(C=10, probability=True), n_jobs=-1))])\n",
    "\n",
    "logistic_clf = Pipeline([('vect', CountVectorizer(ngram_range=(1,2), min_df=4, max_df=0.5, max_features=15000)),\n",
    "                         ('tfidf', TfidfTransformer(use_idf=True)),\n",
    "                       ('clf', OneVsRestClassifier(LogisticRegression(C=0.1, class_weight='balanced'), n_jobs=-1))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters, specify for which part of pipeline with prefix, e.g. 'vect__'\n",
    "SVC_params = {'vect__ngram_range': [(1,2)],\n",
    "              'tfidf__use_idf': [True],\n",
    "              'clf__estimator__C':[0.1, 1, 10]}\n",
    "\n",
    "logistic_params = {#'vect__ngram_range': [(1,2)],\n",
    "                   'vect__min_df': [3, 4, 5, 6],\n",
    "                   'vect__max_df': [0.3, 0.4, 0.5, 0.6],\n",
    "                   #'vect__max_features': [25000, 50000, 100000, None],\n",
    "                    #'vect__max_features': [5000, 7500, 10000, 12500],\n",
    "                  #'tfidf__use_idf': [True],\n",
    "                  #'clf__estimator__C':[0.1, 0.3, 0.6, 1, 3],\n",
    "                  #'clf__estimator__class_weight':['balanced', None],\n",
    "                  #'clf__estimator__penalty':['l1', 'l2']\n",
    "                  }\n",
    "\n",
    "NB_params = {'vect__ngram_range': [(1,2)],\n",
    "              'tfidf__use_idf': [True],\n",
    "              'clf__estimator__alpha':[0.1, 1, 10]}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "TO_TEST = logistic_clf\n",
    "PARAMS = logistic_params\n",
    "\n",
    "gs_clf = GridSearchCV(TO_TEST, PARAMS, scoring='roc_auc', n_jobs=-1, verbose=1, return_train_score=True).fit(X, y)\n",
    "results = pd.DataFrame(gs_clf.cv_results_).sort_values('rank_test_score')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New pipeline with optimal parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pipeline([('vect', CountVectorizer(ngram_range=(1,2), min_df=6, max_df=0.3, max_features=25000)),\n",
    "                         ('tfidf', TfidfTransformer(use_idf=True)),\n",
    "                       ('clf', OneVsRestClassifier(LogisticRegression(C=0.1, class_weight='balanced'), n_jobs=-1))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models to test\n",
    "models = {'Logistic regression': logistic_clf,\n",
    "          #'SVC': SVC_clf,\n",
    "         #'Naïve Bayes': NB_clf\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark score (in case of all 0 predictions):\n",
    "pred = np.zeros(y_test.shape)\n",
    "roc_auc_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for model_name, model in models.items():\n",
    "    print('Training {}...'.format(model_name))\n",
    "    clf = model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = clf.predict(X_train)\n",
    "    print('{} train ROC_AUC score: {}'.format(model_name, roc_auc_score(y_train, y_pred)))\n",
    "    \n",
    "    y_pred = clf.predict(X_test)\n",
    "    print('{} test ROC_AUC score: {}'.format(model_name, roc_auc_score(y_test, y_pred)))\n",
    "    print('{} cross validation ROC_AUC score on 5 folds: {}'.format(model_name, cross_val_score(model, X, y, scoring='roc_auc', cv=5, n_jobs=-1).mean()))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation log"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Base model:\n",
    "logistic_clf = Pipeline([('vect', CountVectorizer(ngram_range=(1,2), min_df=4, max_df=0.5)),\n",
    "                         ('tfidf', TfidfTransformer(use_idf=True)),\n",
    "                       ('clf', OneVsRestClassifier(LogisticRegression(C=10, class_weight='balanced'), n_jobs=-1))])\n",
    "                       \n",
    "Training Logistic regression... \n",
    "Logistic regression train score: 0.9962698309423336\n",
    "Logistic regression test score: 0.8405576601146745\n",
    "\n",
    "Added parameter of max_features at 50000 to CountVectorizer:\n",
    "Training Logistic regression...\n",
    "Logistic regression train score: 0.9943029355921243\n",
    "Logistic regression test score: 0.8474675853683019                      \n",
    "\n",
    "Low regularization with C=100:\n",
    "Training Logistic regression...\n",
    "Logistic regression train score: 0.9979831115997314\n",
    "Logistic regression test score: 0.8037867925850946\n",
    "\n",
    "Higher regularization with C=1:\n",
    "Training Logistic regression...\n",
    "Logistic regression train score: 0.9853433595795452\n",
    "Logistic regression test score: 0.8915858868129986\n",
    "\n",
    "Even higher regularization with C=0.1:\n",
    "Training Logistic regression...\n",
    "Logistic regression train score: 0.9573338881138563\n",
    "Logistic regression test score: 0.9002644623375343\n",
    "\n",
    "Absurdly high regularization with C=0.01:\n",
    "Training Logistic regression...\n",
    "Logistic regression train score: 0.9156204424937071\n",
    "Logistic regression test score: 0.8828513713932437\n",
    "\n",
    "Reg C=0.1, max_features=25000, min_df=3\n",
    "Training Logistic regression...\n",
    "Logistic regression train ROC_AUC score: 0.9560426728378003\n",
    "Logistic regression test ROC_AUC score: 0.9039801868847009\n",
    "\n",
    "max_features=12500\n",
    "Training Logistic regression...\n",
    "Logistic regression train ROC_AUC score: 0.9537134328359774\n",
    "Logistic regression test ROC_AUC score: 0.9083269187748823\n",
    "Logistic regression cross validation ROC_AUC score on 5 folds: 0.975445455672514\n",
    "\n",
    "max_features=15000\n",
    "Training Logistic regression...\n",
    "Logistic regression train ROC_AUC score: 0.9545163453977098\n",
    "Logistic regression test ROC_AUC score: 0.9078888840008452\n",
    "Logistic regression cross validation ROC_AUC score on 5 folds: 0.9756952727924262"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final parameter tuning (manual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_clf = Pipeline([('vect', CountVectorizer(ngram_range=(1,2), min_df=6, max_df=0.3, max_features=25000)),\n",
    "                         ('tfidf', TfidfTransformer(use_idf=True)),\n",
    "                       ('clf', OneVsRestClassifier(LogisticRegression(C=0.1, class_weight='balanced'), n_jobs=-1))])\n",
    "\n",
    "print('Cross_val_score with C=0.1, max_features=25000, max_df=0.3, min_df=6: ', cross_val_score(logistic_clf, X, y, scoring='roc_auc', cv=5, n_jobs=-1).mean())\n",
    "\n",
    "logistic_clf = Pipeline([('vect', CountVectorizer(ngram_range=(1,2), min_df=5, max_df=0.3, max_features=25000)),\n",
    "                         ('tfidf', TfidfTransformer(use_idf=True)),\n",
    "                       ('clf', OneVsRestClassifier(LogisticRegression(C=0.1, class_weight='balanced'), n_jobs=-1))])\n",
    "\n",
    "print('Cross_val_score with C=0.1, max_features=25000, max_df=0.4, min_df=5: ', cross_val_score(logistic_clf, X, y, scoring='roc_auc', cv=5, n_jobs=-1).mean())\n",
    "\n",
    "#Cross_val_score with C=0.1:  0.9759436463859725\n",
    "#Cross_val_score with C=0.01:  0.9635830622046198\n",
    "\n",
    "#Cross_val_score with C=0.1, max_features=25000, max_df=0.5, min_df=4:  0.9760136616571229\n",
    "#Cross_val_score with C=0.1, max_features=50000, max_df=0.5, min_df=4:  0.9759436463859725\n",
    "\n",
    "#Cross_val_score with C=0.1, max_features=25000, max_df=0.3, min_df=4:  0.9762885351187846\n",
    "#Cross_val_score with C=0.1, max_features=25000, max_df=0.4, min_df=4:  0.9760047347166096\n",
    "\n",
    "#Cross_val_score with C=0.1, max_features=25000, max_df=0.3, min_df=6:  0.9763128416184881\n",
    "#Cross_val_score with C=0.1, max_features=25000, max_df=0.3, min_df=5:  0.976302536076927\n",
    "\n",
    "# Same test, but with correction in clean_text function (fixed lowercase issue):\n",
    "# Cross_val_score with C=0.1, max_features=25000, max_df=0.3, min_df=6:  0.977167248668523\n",
    "# Cross_val_score with C=0.1, max_features=25000, max_df=0.4, min_df=5:  0.9771515047562769"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do: write automated test function (start with default, set params to best so far, test one param per iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declare model with final parameters to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = Pipeline([('vect', CountVectorizer(ngram_range=(1,2), min_df=6, max_df=0.3, max_features=25000)),\n",
    "                         ('tfidf', TfidfTransformer(use_idf=True)),\n",
    "                       ('clf', OneVsRestClassifier(LogisticRegression(C=0.1, class_weight='balanced'), n_jobs=-1))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions for submission and save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: submission should be probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_data.comment_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean test data\n",
    "test_data['comment_text'] = test_data['comment_text'].apply(lambda x: clean_text(x, normalization='lemmatization'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_data.comment_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_final = final_model.fit(X, y).predict_proba(test_data['comment_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.DataFrame(y_pred_final, columns=y_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.concat([test_data['id'], predictions], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('ID 00001cee341fdb12:\\n',test_data.comment_text[0], '\\n')\n",
    "print('ID 0000247867823ef7:\\n',test_data.comment_text[1])\n",
    "submission.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_PATH = './data/submission_simple.csv'\n",
    "submission.to_csv(TARGET_PATH, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My highest public leaderboard score on Kaggle: 0.0.9723 (Good for 3081nd place)\n",
    "\n",
    "No. 1 score on the leaderboard: 0.9889"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus: most important words per label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_estimators = final_model.named_steps['clf'].estimators_\n",
    "vocab = final_model.named_steps['vect'].vocabulary_\n",
    "index_to_words = {value: key for key,value in vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, label in enumerate(y.columns):\n",
    "    print('Current label: {}'.format(label))\n",
    "    words = {}\n",
    "    coefs = all_estimators[index].coef_[0]\n",
    "    for key in index_to_words.keys():\n",
    "        words[index_to_words[key]] = coefs[key]\n",
    "    words = sorted(words.items(), key=lambda x:x[1], reverse=True)\n",
    "    top_5_most_important = words[:10]\n",
    "    top_5_least_important = words[-10:]\n",
    "    print('Top 10 most {} words:'.format(label))\n",
    "    for pair in top_5_most_important:\n",
    "        print(pair)\n",
    "    print('')\n",
    "    print('Top 10 least {} words:'.format(label))\n",
    "    for pair in top_5_least_important:\n",
    "        print(pair)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
